{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the link for each author\n",
    "authors = pd.read_csv('./data/data_ukiyo-e_authors.csv')\n",
    "authors = authors['author_link'].tolist()\n",
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_artist_page(artist_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'\n",
    "    }\n",
    "\n",
    "    while artist_url:\n",
    "        response = requests.get(artist_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        for img_div in soup.find_all('div', class_='img'):\n",
    "            link = img_div.find('a', class_='img')['href']\n",
    "            title = img_div.find('a', class_='img')['title']\n",
    "            artist = img_div.find('a', class_='artist').get_text(strip=True)\n",
    "            # print(f'Link: {link}\\nTitle: {title}\\nArtist: {artist}')\n",
    "            yield {\n",
    "                'Link': link,\n",
    "                'Title': title,\n",
    "                'Artist': artist\n",
    "            }\n",
    "\n",
    "        next_page = soup.find('span', class_='next')\n",
    "        if next_page:\n",
    "            artist_url = next_page.find('a')['href']\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl all the artwork links\n",
    "fieldnames = ['Link', 'Title', 'Artist']\n",
    "\n",
    "with open('./data/ukiyo-e_artworks.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, artist_url in enumerate(authors):\n",
    "        for artwork_data in crawl_artist_page(artist_url):\n",
    "            writer.writerow(artwork_data)\n",
    "        print(f'{i+1}/150, Finished crawling {artist_url}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_artwork_details(artwork_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'\n",
    "    }\n",
    "\n",
    "    response = requests.get(artwork_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    try:\n",
    "        # scaled image\n",
    "        image_url = soup.find('div', class_='imageholder').find('img')['src']\n",
    "        # large image\n",
    "        # image_url = soup.find('div', class_='imageholder').find('a')['href']\n",
    "    except:\n",
    "        image_url = None\n",
    "    try:\n",
    "        artist = soup.find('p', class_='row artist').find('a').get_text(strip=True)\n",
    "    except:\n",
    "        artist = None\n",
    "    try:\n",
    "        title = soup.find('p', class_='row title').find('span').get_text(strip=True)\n",
    "    except:\n",
    "        title = None\n",
    "    try:\n",
    "        date = soup.find('p', class_='row date').find('span').get_text(strip=True)\n",
    "    except:\n",
    "        date = None\n",
    "    try:\n",
    "        details = soup.find('p', class_='row details').find('a')['href']\n",
    "    except:\n",
    "        details = None\n",
    "    try:\n",
    "        source = soup.find('p', class_='row source').find('a')['href']\n",
    "    except:\n",
    "        source = None\n",
    "    try:\n",
    "        description_label = soup.find('strong', string='Description:')\n",
    "        description_paragraph = description_label.find_parent('p', class_='row').find_next_sibling('p', class_='row')\n",
    "        description = description_paragraph.find('span', class_='col-xs-9').get_text(strip=True)\n",
    "    except:\n",
    "        description = None\n",
    "    try:\n",
    "        similar_prints = [\n",
    "            img_div.find('a')['href']\n",
    "            for img_div in soup.find_all('div', class_='img')\n",
    "        ]\n",
    "    except:\n",
    "        similar_prints = None\n",
    "\n",
    "    # print(f'Image URL: {image_url}\\nArtist: {artist}\\nTitle: {title}\\nDate: {date}\\nDetails: {details}\\nSource: {source}\\nDescription: {description}\\nSimilar Prints: {similar_prints}')\n",
    "    yield {\n",
    "        'Image URL': image_url,\n",
    "        'Artist': artist,\n",
    "        'Title': title,\n",
    "        'Date': date,\n",
    "        'Details': details,\n",
    "        'Source': source,\n",
    "        'Description': description,\n",
    "        'Similar Prints': similar_prints\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177985\n"
     ]
    }
   ],
   "source": [
    "# read the artwork links\n",
    "artworks = pd.read_csv('./data/ukiyo-e_artworks.csv')\n",
    "artworks = artworks['Link'].tolist()\n",
    "print(len(artworks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/177985, Finished crawling https://ukiyo-e.org/image/ritsumei/Z0166-287\n",
      "10000/177985, Finished crawling https://ukiyo-e.org/image/mfa/sc157376\n",
      "15000/177985, Finished crawling https://ukiyo-e.org/image/artelino/43707g1\n",
      "20000/177985, Finished crawling https://ukiyo-e.org/image/bm/AN00515121_001_l\n",
      "25000/177985, Finished crawling https://ukiyo-e.org/image/waseda/100-6828\n",
      "30000/177985, Finished crawling https://ukiyo-e.org/image/metro/N280-002\n",
      "35000/177985, Finished crawling https://ukiyo-e.org/image/waseda/118-0087\n",
      "40000/177985, Finished crawling https://ukiyo-e.org/image/waseda/002-0352\n",
      "45000/177985, Finished crawling https://ukiyo-e.org/image/bm/AN00799763_001_l\n",
      "50000/177985, Finished crawling https://ukiyo-e.org/image/mak/11486-52\n",
      "55000/177985, Finished crawling https://ukiyo-e.org/image/jaodb/Kunisada_1_Utagawa-Ukiyo_e_Comparison_of_Genji-CH5_Wakamurasaki-00030017-020302-F06\n",
      "60000/177985, Finished crawling https://ukiyo-e.org/image/artelino/11335g1\n",
      "65000/177985, Finished crawling https://ukiyo-e.org/image/met/DP120445\n",
      "70000/177985, Finished crawling https://ukiyo-e.org/image/mfa/sc206759\n",
      "75000/177985, Finished crawling https://ukiyo-e.org/image/mak/10505-5\n",
      "80000/177985, Finished crawling https://ukiyo-e.org/image/waseda/100-0027\n",
      "85000/177985, Finished crawling https://ukiyo-e.org/image/waseda/005-0404\n",
      "90000/177985, Finished crawling https://ukiyo-e.org/image/waseda/001-0185\n",
      "95000/177985, Finished crawling https://ukiyo-e.org/image/bm/AN00703951_001_l\n",
      "100000/177985, Finished crawling https://ukiyo-e.org/image/ohmi/Hokusai_Katsushika-36_Fuji-Yuyudo-Inume_Touge-01-07-07-2007-8799-x2000\n",
      "105000/177985, Finished crawling https://ukiyo-e.org/image/mia/31095\n",
      "110000/177985, Finished crawling https://ukiyo-e.org/image/harvard/HUAM-CARP06933\n",
      "115000/177985, Finished crawling https://ukiyo-e.org/image/waseda/016-1252\n",
      "120000/177985, Finished crawling https://ukiyo-e.org/image/waseda/007-2178\n",
      "125000/177985, Finished crawling https://ukiyo-e.org/image/waseda/500-2045\n",
      "130000/177985, Finished crawling https://ukiyo-e.org/image/mfa/sc144422\n",
      "135000/177985, Finished crawling https://ukiyo-e.org/image/jaodb/Chikanobu_Yoshu-Plum_Trees_In_The_Garden-Autumn-00035576-040419-F06\n",
      "140000/177985, Finished crawling https://ukiyo-e.org/image/ritsumei/arcUP0764\n",
      "145000/177985, Finished crawling https://ukiyo-e.org/image/robynbuntin/8655-4\n",
      "150000/177985, Finished crawling https://ukiyo-e.org/image/metro/N132-003\n",
      "155000/177985, Finished crawling https://ukiyo-e.org/image/robynbuntin/3091-3\n",
      "160000/177985, Finished crawling https://ukiyo-e.org/image/jaodb/Kawase_Hasui-No_Series-Cormorant_Fishing_on_Nagaragawa-00037326-050204-F12\n",
      "165000/177985, Finished crawling https://ukiyo-e.org/image/artelino/33887g1\n",
      "170000/177985, Finished crawling https://ukiyo-e.org/image/artelino/19174g1\n",
      "175000/177985, Finished crawling https://ukiyo-e.org/image/ohmi/Ikeda_Shuzo-Parade-02-01-02-2007-8123-x2000\n"
     ]
    }
   ],
   "source": [
    "# for each artwork, extract the details\n",
    "fieldnames = ['Image URL', 'Artist', 'Title', 'Date', 'Details', 'Source', 'Description', 'Similar Prints']\n",
    "\n",
    "with open(f'./data/ukiyo-e_artworks_details.csv', mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, artwork_url in enumerate(artworks):\n",
    "        for artwork_data in extract_artwork_details(artwork_url):\n",
    "            writer.writerow(artwork_data)\n",
    "\n",
    "        if (i+1) % 5000 == 0:\n",
    "            print(f'{i+1}/{len(artworks)}, Finished crawling {artwork_url}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
