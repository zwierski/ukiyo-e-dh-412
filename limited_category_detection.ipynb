{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchvision\n",
    "! pip install -U git+https://github.com/luca-medeiros/lang-segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "from lang_sam import LangSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data containing the time perriods\n",
    "data = pd.read_csv('data_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Image URL', 'Artist', 'Title', 'Date', 'Details',\n",
       "       'Source', 'Description', 'Similar Prints', 'Description_length',\n",
       "       'Nouns', 'Year', 'Period'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['Image URL'])\n",
    "data['Image URL'] = data['Image URL'].apply(lambda x: x.replace('scaled', 'images'))\n",
    "def get_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompts = ['Emperor', 'Shogun', 'Government officials', 'Voting', 'Constitution',\n",
    "              'Courtroom', 'Legal document', 'Contracts', 'Patent', 'Judge', 'Lawyer','Police', 'Prison',\n",
    "              'School', 'School uniform', 'Textbooks', 'Scientific instrument',\n",
    "              'Steamship', 'Telegraph poles', 'Brick house',\n",
    "              'Factory', 'Steam-powered machinery', 'Worker', 'Railway', 'Train', 'Train station',\n",
    "              'Soldier', 'Gun', 'Military uniform', 'Warship',\n",
    "              'Kimono', 'Western style clothing', 'Gloves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'PeriodImage URL', 'Emperor', 'Shogun', 'Government officials',\n",
       "       'Voting', 'Constitution', 'Courtroom', 'Legal document', 'Contracts',\n",
       "       'Patent', 'Judge', 'Lawyer', 'Police', 'Prison', 'School',\n",
       "       'School uniform', 'Textbooks', 'Scientific instrument', 'Steamship',\n",
       "       'Telegraph poles', 'Brick house', 'Factory', 'Steam-powered machinery',\n",
       "       'Worker', 'Railway', 'Train', 'Train station', 'Soldier', 'Gun',\n",
       "       'Military uniform', 'Warship', 'Kimono', 'Western style clothing',\n",
       "       'Gloves'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the datafram to store the results\n",
    "result = pd.DataFrame(columns=['Year', 'Period' 'Image URL']+text_prompts)\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from C:\\Users\\Ke\\.cache\\huggingface\\hub\\models--ShilongLiu--GroundingDINO\\snapshots\\a94c9b567a2a374598f05c584e96798a170c56fb\\groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\transformers\\modeling_utils.py:874: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "d:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Workspace\\课程\\history and digital\\ukiyo-e-dh-412\\limited_category_detection.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/%E8%AF%BE%E7%A8%8B/history%20and%20digital/ukiyo-e-dh-412/limited_category_detection.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# predict the text prompts\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/%E8%AF%BE%E7%A8%8B/history%20and%20digital/ukiyo-e-dh-412/limited_category_detection.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m text_prompts:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Workspace/%E8%AF%BE%E7%A8%8B/history%20and%20digital/ukiyo-e-dh-412/limited_category_detection.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     masks, boxes, phrases, logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(image, prompt)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Workspace/%E8%AF%BE%E7%A8%8B/history%20and%20digital/ukiyo-e-dh-412/limited_category_detection.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     result\u001b[39m.\u001b[39mloc[i, prompt] \u001b[39m=\u001b[39m boxes\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\lang_sam\\lang_sam.py:106\u001b[0m, in \u001b[0;36mLangSAM.predict\u001b[1;34m(self, image_pil, text_prompt, box_threshold, text_threshold)\u001b[0m\n\u001b[0;32m    104\u001b[0m masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([])\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(boxes) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_sam(image_pil, boxes)\n\u001b[0;32m    107\u001b[0m     masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m masks, boxes, phrases, logits\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\lang_sam\\lang_sam.py:92\u001b[0m, in \u001b[0;36mLangSAM.predict_sam\u001b[1;34m(self, image_pil, boxes)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_sam\u001b[39m(\u001b[39mself\u001b[39m, image_pil, boxes):\n\u001b[0;32m     91\u001b[0m     image_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(image_pil)\n\u001b[1;32m---> 92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msam\u001b[39m.\u001b[39;49mset_image(image_array)\n\u001b[0;32m     93\u001b[0m     transformed_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msam\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mapply_boxes_torch(boxes, image_array\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m])\n\u001b[0;32m     94\u001b[0m     masks, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msam\u001b[39m.\u001b[39mpredict_torch(\n\u001b[0;32m     95\u001b[0m         point_coords\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m         point_labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m         boxes\u001b[39m=\u001b[39mtransformed_boxes\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msam\u001b[39m.\u001b[39mdevice),\n\u001b[0;32m     98\u001b[0m         multimask_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     99\u001b[0m     )\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\segment_anything\\predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[1;34m(self, image, image_format)\u001b[0m\n\u001b[0;32m     57\u001b[0m input_image_torch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(input_image, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     58\u001b[0m input_image_torch \u001b[39m=\u001b[39m input_image_torch\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()[\u001b[39mNone\u001b[39;00m, :, :, :]\n\u001b[1;32m---> 60\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_torch_image(input_image_torch, image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39;49m])\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\segment_anything\\predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[1;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(transformed_image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[0;32m     88\u001b[0m input_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpreprocess(transformed_image)\n\u001b[1;32m---> 89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mimage_encoder(input_image)\n\u001b[0;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_image_set \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\segment_anything\\modeling\\image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    109\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed\n\u001b[0;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m--> 112\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[0;32m    114\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\segment_anything\\modeling\\image_encoder.py:180\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    177\u001b[0m     x \u001b[39m=\u001b[39m window_unpartition(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, pad_hw, (H, W))\n\u001b[0;32m    179\u001b[0m x \u001b[39m=\u001b[39m shortcut \u001b[39m+\u001b[39m x\n\u001b[1;32m--> 180\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(x))\n\u001b[0;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\segment_anything\\modeling\\common.py:26\u001b[0m, in \u001b[0;36mMLPBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin1(x)))\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\APPS\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LangSAM()\n",
    "for i in range(1):\n",
    "    # get the year, period, and image url\n",
    "    result.loc[i, 'Year'] = data['Year'].iloc[i]\n",
    "    result.loc[i, 'Period'] = data['Period'].iloc[i]\n",
    "    result.loc[i, 'Image URL'] = data['Image URL'].iloc[i]\n",
    "\n",
    "    # get the image\n",
    "    url = data['Image URL'].iloc[i]\n",
    "    image = get_image(url)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # predict the text prompts\n",
    "    for prompt in text_prompts:\n",
    "        masks, boxes, phrases, logits = model.predict(image, prompt)\n",
    "        result.loc[i, prompt] = boxes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "result.to_csv('limited_category_detection.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
